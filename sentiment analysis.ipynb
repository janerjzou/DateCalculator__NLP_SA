{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "88051c3d-4a44-4ec6-8adb-c1d38653e94d",
    "_uuid": "71fff663eb9075c63db6ddd0904379ffbcdcad20"
   },
   "source": [
    "# Machine Learning Challenge\n",
    "\n",
    "## Overview\n",
    "\n",
    "The focus of this exercise is on a field within machine learning called [Natural Language Processing](https://en.wikipedia.org/wiki/Natural-language_processing). We can think of this field as the intersection between language, and machine learning. Tasks in this field include automatic translation (Google translate), intelligent personal assistants (Siri), information extraction, and speech recognition for example.\n",
    "\n",
    "NLP uses many of the same techniques as traditional data science, but also features a number of specialised skills and approaches. There is no expectation that you have any experience with NLP, however, to complete the challenge it will be useful to have the following skills:\n",
    "\n",
    "- understanding of the python programming language\n",
    "- understanding of basic machine learning concepts, i.e. supervised learning\n",
    "\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Download this notebook!\n",
    "2. Answer each of the provided questions, including your source code as cells in this notebook.\n",
    "3. Share the results with us, e.g. a Github repo.\n",
    "\n",
    "### Task description\n",
    "\n",
    "You will be performing a task known as [sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis). Here, the goal is to predict sentiment -- the emotional intent behind a statement -- from text. For example, the sentence: \"*This movie was terrible!\"* has a negative sentiment, whereas \"*loved this cinematic masterpiece*\" has a positive sentiment.\n",
    "\n",
    "To simplify the task, we consider sentiment binary: labels of `1` indicate a sentence has a positive sentiment, and labels of `0` indicate that the sentence has a negative sentiment.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "The dataset is split across three files, representing three different sources -- Amazon, Yelp and IMDB. Your task is to build a sentiment analysis model using both the Yelp and IMDB data as your training-set, and test the performance of your model on the Amazon data.\n",
    "\n",
    "Each file can be found in the `input` directory, and contains 1000 rows of data. Each row contains a sentence, a `tab` character and then a label -- `0` or `1`. \n",
    "\n",
    "**Notes**\n",
    "- Feel free to use existing machine learning libraries as components in you solution!\n",
    "- Suggested libraries: `sklearn` (for machine learning), `pandas` (for loading/processing data), `spacy` (for text processing).\n",
    "- As mentioned, you are not expected to have previous experience with this exact task. You are free to refer to external tutorials/resources to assist you. However, you will be asked to justfify the choices you have made -- so make you understand the approach you have taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amazon_cells_labelled.txt', 'yelp_labelled.txt', 'imdb_labelled.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(\"./input\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So there is no way for me to plug it in here in the US unless I go by a converter.\t0\r\n",
      "Good case, Excellent value.\t1\r\n",
      "Great for the jawbone.\t1\r\n",
      "Tied to charger for conversations lasting more than 45 minutes.MAJOR PROBLEMS!!\t0\r\n",
      "The mic is great.\t1\r\n",
      "I have to jiggle the plug to get it to line up right to get decent volume.\t0\r\n",
      "If you have several dozen or several hundred contacts, then imagine the fun of sending each of them one by one.\t0\r\n",
      "If you are Razr owner...you must have this!\t1\r\n",
      "Needless to say, I wasted my money.\t0\r\n",
      "What a waste of money and time!.\t0\r\n"
     ]
    }
   ],
   "source": [
    "!head \"./input/amazon_cells_labelled.txt\"\n",
    "# !head \"./input/imdb_labelled.txt\"\n",
    "# !head \"./input/yelp_labelled.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "387106cd-e89a-462f-b204-a91a73d12137",
    "_uuid": "cbd1a4b1d16ce7db6def7b3b393b48618d7e4777"
   },
   "source": [
    "# Tasks\n",
    "### 1. Read and concatenate data into test and train sets.\n",
    "### 2. Prepare the data for input into your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a8240a39-7002-435b-ba45-ac859d209f7f",
    "_uuid": "69c6d7ea240a191abfaaf00574f09521944387d7"
   },
   "source": [
    "#### 2a: Find the ten most frequent words in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f4cc399e-66c4-4bf7-a8e1-03711372c7b4",
    "_uuid": "eb8b033dc4a841702ae52d4ec71e7718b3257dda"
   },
   "source": [
    "### 3. Train your model and justify your choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1a5840b2-c84c-42f6-9fc9-4fed64e48298",
    "_uuid": "f4eeecd64b54cc05098affe6cca4c40204af8ecf"
   },
   "source": [
    "### 4. Evaluate your model using metric(s) you see fit and justify your choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read and concatenate data into test and train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wow... Loved this place.\\t1',\n",
       " 'Crust is not good.\\t0',\n",
       " 'Not tasty and the texture was just nasty.\\t0',\n",
       " 'Stopped by during the late May bank holiday off Rick Steve recommendation and loved it.\\t1']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp = open('./input/yelp_labelled.txt',\"r\", encoding = \"ISO-8859-1\") # Yelp txt\n",
    "yelp_s = yelp.readlines()\n",
    "yelp_s = [item.strip() for item in yelp_s]\n",
    "\n",
    "yelp_s[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A very, very, very slow-moving, aimless movie about a distressed, drifting young man.  \\t0',\n",
       " 'Not sure who was more lost - the flat characters or the audience, nearly half of whom walked out.  \\t0',\n",
       " 'Attempting artiness with black & white and clever camera angles, the movie disappointed - became even more ridiculous - as the acting was poor and the plot and lines almost non-existent.  \\t0',\n",
       " 'Very little music or anything to speak of.  \\t0']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb = open('./input/imdb_labelled.txt',\"r\", encoding = \"ISO-8859-1\") # IMDB txt\n",
    "imdb_s = imdb.readlines()\n",
    "imdb_s = [item.strip() for item in imdb_s]\n",
    "\n",
    "imdb_s[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['So there is no way for me to plug it in here in the US unless I go by a converter.\\t0',\n",
       " 'Good case, Excellent value.\\t1',\n",
       " 'Great for the jawbone.\\t1',\n",
       " 'Tied to charger for conversations lasting more than 45 minutes.MAJOR PROBLEMS!!\\t0']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon = open('./input/amazon_cells_labelled.txt',\"r\", encoding = \"ISO-8859-1\") # IMDB txt\n",
    "amazon_s = amazon.readlines()\n",
    "amazon_s = [item.strip() for item in amazon_s]\n",
    "\n",
    "amazon_s[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000, 1000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(yelp_s),len(imdb_s), len(amazon_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split sentencs & label for yelp\n",
    "yelp_text = []\n",
    "yelp_label = []\n",
    "for i in yelp_s:\n",
    "    text, st = i.split('\\t')\n",
    "    yelp_text.append(text)\n",
    "    yelp_label.append(int(st))\n",
    "len(yelp_text),len(yelp_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split sentencs & label for imdb\n",
    "imdb_text = []\n",
    "imdb_label = []\n",
    "for i in imdb_s:\n",
    "    text, st = i.split('  \\t')\n",
    "    imdb_text.append(text)\n",
    "    imdb_label.append(int(st))\n",
    "len(imdb_text),len(imdb_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split sentencs & label for amazon\n",
    "amazon_text = []\n",
    "amazon_label = []\n",
    "for i in amazon_s:\n",
    "    text, st = i.split('\\t')\n",
    "    amazon_text.append(text)\n",
    "    amazon_label.append(int(st))\n",
    "len(amazon_text),len(amazon_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2000, 1000, 1000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text = yelp_text + imdb_text     # (2000 items) ['Wow... Loved this pl...', 'Crust is not good.',..]\n",
    "train_label = yelp_label + imdb_label  # (2000 items) [1, 0, 0, 1, 1, ...]\n",
    "\n",
    "test_text = amazon_text     # (1000 items) ['So there is no way f...', 'Good case, Excellent...',...]\n",
    "test_label = amazon_label   # (1000 items) [0, 1, 1, 0, 1, ...]\n",
    "\n",
    "len(train_text), len(train_label), len(test_text), len(test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Prepare the data for input into your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jane/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/jane/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jane/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# import spacy\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# # from nltk.stem.porter import PorterStemmer\n",
    "# # porter = PorterStemmer()\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV #for best parameters\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stop_words = sw.words()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(sentences):\n",
    "    text_lower = [t.lower() for t in sentences]                      # lowercase\n",
    "    text_punc  = [re.sub(r\"[^a-z0-9]+\", \" \", i) for i in text_lower] # remove punc\n",
    "    text_punc  = [re.sub(r\"[0-9]\",\"\", i) for i in text_punc]         # remove digits\n",
    "    text_token = [word_tokenize(i) for i in text_punc]               # tokenisation\n",
    "    \n",
    "    # remove stop words - to, the, is, have....\n",
    "    text_ns=[] \n",
    "    for tokens in text_token:\n",
    "        filtered_sentence = [w for w in tokens if not w in stop_words]\n",
    "        text_ns.append(filtered_sentence)\n",
    "    \n",
    "    # lemmatisation - prices->price, kits->kit ...\n",
    "    text_lemma = []\n",
    "    for tokens in text_ns:\n",
    "        lemma_sentence = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "        text_lemma.append(lemma_sentence)\n",
    "    \n",
    "    # # stemming\n",
    "    # text_stem = []\n",
    "    # for tokens in text_ns:\n",
    "    #     stem = [porter.stem(w) for w in tokens]\n",
    "    #     text_stem.append(stem)    \n",
    "\n",
    "    return text_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tt = prepare_data(train_text)\n",
    "test_tt = prepare_data(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_string = [' '.join(tokens) for tokens in train_tt]\n",
    "test_string = [' '.join(tokens) for tokens in test_tt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert text into tf-idf values\n",
    "\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "X_train_bigram = bigram_vectorizer.fit_transform(train_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigram with tf-idf\n",
    "\n",
    "bigram_tf_idf_transformer = TfidfTransformer()\n",
    "X_train_bigram_tf_idf = bigram_tf_idf_transformer.fit_transform(X_train_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2000x13410 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 22704 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bigram_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1600, 13410), (400, 13410), 1600, 400)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_split(X_train_bigram_tf_idf, y, test_size=0.2, random_state=1)\n",
    "\n",
    "X_train.shape, X_test.shape, len(y_train), len(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2a: Find the ten most frequent words in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12533"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list =  []  # 12533 words \n",
    "for item in train_tt: \n",
    "    for word in item:\n",
    "        word_list.append(word)\n",
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('movie', 212),\n",
       " ('film', 187),\n",
       " ('good', 153),\n",
       " ('food', 127),\n",
       " ('place', 118),\n",
       " ('great', 111),\n",
       " ('time', 103),\n",
       " ('like', 97),\n",
       " ('bad', 89),\n",
       " ('service', 87)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = Counter(word_list)\n",
    "most_frequent_words = count.most_common(10)\n",
    "\n",
    "most_frequent_words  # top 10 frequent words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train your model and justify your choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   15.8s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   55.9s finished\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression model\n",
    "lr = LogisticRegressionCV(cv=100,scoring='accuracy',random_state=0,n_jobs=-1,verbose=3,max_iter=500).fit(X_train,y_train)\n",
    "\n",
    "# put IMDB & Yelp data into LR model for training\n",
    "y_pred = lr.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.79      0.81       205\n",
      "           1       0.79      0.83      0.81       195\n",
      "\n",
      "    accuracy                           0.81       400\n",
      "   macro avg       0.81      0.81      0.81       400\n",
      "weighted avg       0.81      0.81      0.81       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get model evaluation\n",
    "print(classification_report(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare test data (AMAZON) for evaluation\n",
    "\n",
    "test_data_vec = bigram_vectorizer.transform(test_string)\n",
    "test_data = bigram_tf_idf_transformer.transform(test_data_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put amazon data into the trained LR model and get result\n",
    "test_pred = lr.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.74      0.75       514\n",
      "           1       0.73      0.76      0.74       486\n",
      "\n",
      "    accuracy                           0.75      1000\n",
      "   macro avg       0.75      0.75      0.75      1000\n",
      "weighted avg       0.75      0.75      0.75      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_pred ,test_label))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
